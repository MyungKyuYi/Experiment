{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "008a259e-bcea-45b2-a335-1bbe0591e50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"E:/dataset/Hypertension/MIMIC-III/MIMIC3.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab485a72-4e9d-4af8-8e99-c4845621929a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda33\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 213\u001b[0m\n\u001b[0;32m    211\u001b[0m     z1, z2 \u001b[38;5;241m=\u001b[39m model_ssl(v1, v2)\n\u001b[0;32m    212\u001b[0m     loss \u001b[38;5;241m=\u001b[39m nt_xent(z1, z2)\n\u001b[1;32m--> 213\u001b[0m     optimizer_ssl\u001b[38;5;241m.\u001b[39mzero_grad(); loss\u001b[38;5;241m.\u001b[39mbackward(); optimizer_ssl\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    214\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[SSL] Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m — Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(ssl_loader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda33\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    628\u001b[0m )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda33\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda33\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -------------------- Seed 고정 --------------------\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_seed()\n",
    "\n",
    "# -------------------- 1. Augmentation --------------------\n",
    "def jitter(x, sigma=0.01):\n",
    "    return x + sigma * np.random.randn(*x.shape)\n",
    "\n",
    "def scaling(x, sigma=0.05):\n",
    "    return x * np.random.normal(1.0, sigma)\n",
    "\n",
    "class PPGAugmentation:\n",
    "    def __call__(self, x):\n",
    "        x = jitter(x)\n",
    "        x = scaling(x)\n",
    "        return x\n",
    "\n",
    "# -------------------- 2. Dual Encoder (CNN + Transformer 병렬) --------------------\n",
    "class CNNBranch(nn.Module):\n",
    "    def __init__(self, out_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(32), nn.ReLU(),\n",
    "            nn.Conv1d(32, 64, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(64), nn.ReLU(),\n",
    "            nn.Conv1d(64, out_dim, kernel_size=7, padding=3),\n",
    "            nn.BatchNorm1d(out_dim), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x.squeeze(-1)\n",
    "\n",
    "class TransformerBranch(nn.Module):\n",
    "    def __init__(self, d_model=128, nhead=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Conv1d(1, d_model, kernel_size=1)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=d_model*2)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)\n",
    "        x = x.permute(2, 0, 1)  # (L, B, d_model)\n",
    "        x = self.transformer(x)\n",
    "        return x.mean(dim=0)\n",
    "\n",
    "class DualBranchEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn_branch = CNNBranch(out_dim=128)\n",
    "        self.trans_branch = TransformerBranch(d_model=128)\n",
    "\n",
    "    def forward(self, x_aug):\n",
    "        a = self.cnn_branch(x_aug)\n",
    "        b = self.trans_branch(x_aug)\n",
    "        return torch.cat([a, b], dim=1)  # (B, 256)\n",
    "\n",
    "# -------------------- 3. Projector Head --------------------\n",
    "class ProjHead(nn.Module):\n",
    "    def __init__(self, input_dim=256, proj_dim=256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, proj_dim),\n",
    "            nn.BatchNorm1d(proj_dim), nn.ReLU(),\n",
    "            nn.Linear(proj_dim, proj_dim),\n",
    "            nn.BatchNorm1d(proj_dim), nn.ReLU(),\n",
    "            nn.Linear(proj_dim, proj_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return F.normalize(self.net(x), dim=1)\n",
    "\n",
    "# -------------------- 4. Full SSL Model --------------------\n",
    "class DualEncoderSSL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = DualBranchEncoder()\n",
    "        self.projector = ProjHead()\n",
    "\n",
    "    def forward(self, v1, v2):\n",
    "        h1 = self.encoder(v1)\n",
    "        h2 = self.encoder(v2)\n",
    "        return self.projector(h1), self.projector(h2)\n",
    "\n",
    "# -------------------- 5. NT-Xent Loss --------------------\n",
    "def nt_xent(z1, z2, temperature=0.5):\n",
    "    B = z1.size(0)\n",
    "    z = torch.cat([z1, z2], dim=0)\n",
    "    sim = F.cosine_similarity(z.unsqueeze(1), z.unsqueeze(0), dim=2) / temperature\n",
    "    mask = ~torch.eye(2*B, device=z.device, dtype=torch.bool)\n",
    "    sim_neg = sim.masked_select(mask).view(2*B, -1)\n",
    "    pos = torch.cat([sim.diagonal(B), sim.diagonal(-B)]).view(2*B, 1)\n",
    "    logits = torch.cat([pos, sim_neg], dim=1)\n",
    "    return F.cross_entropy(logits, torch.zeros(2*B, dtype=torch.long, device=z.device))\n",
    "\n",
    "# -------------------- 6. Dataset --------------------\n",
    "class SSLDataset(Dataset):\n",
    "    def __init__(self, segments, transform):\n",
    "        self.X = segments\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        aug1 = self.transform(x)\n",
    "        aug2 = self.transform(x)\n",
    "        return torch.from_numpy(aug1[None, :]).float(), torch.from_numpy(aug2[None, :]).float()\n",
    "\n",
    "class ClassifierDataset(Dataset):\n",
    "    def __init__(self, segments, labels):\n",
    "        self.X = segments\n",
    "        self.y = labels\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx][None, :]).float(), torch.tensor(self.y[idx]).long()\n",
    "\n",
    "# -------------------- 7. Fine-tune Classifier --------------------\n",
    "class TCNBranch(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, \n",
    "                              padding=(kernel_size-1)//2 * dilation, dilation=dilation)\n",
    "        self.bn = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))\n",
    "\n",
    "class InceptionTCNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.branch1 = TCNBranch(in_channels, out_channels//4, kernel_size=1)\n",
    "        self.branch2 = TCNBranch(in_channels, out_channels//4, kernel_size=3)\n",
    "        self.branch3 = TCNBranch(in_channels, out_channels//4, kernel_size=5)\n",
    "        self.branch4 = TCNBranch(in_channels, out_channels//4, kernel_size=7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.branch1(x)\n",
    "        out2 = self.branch2(x)\n",
    "        out3 = self.branch3(x)\n",
    "        out4 = self.branch4(x)\n",
    "        return torch.cat([out1, out2, out3, out4], dim=1)\n",
    "\n",
    "class FineTuneClassifier(nn.Module):\n",
    "    def __init__(self, encoder, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.inception_tcn = InceptionTCNBlock(1, 256)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=256, nhead=4)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            feat = self.encoder(x)\n",
    "        feat = feat.unsqueeze(-1)\n",
    "        feat = self.inception_tcn(feat)\n",
    "        feat = feat.permute(2, 0, 1)\n",
    "        feat = self.transformer(feat)\n",
    "        feat = feat.squeeze(0)\n",
    "        out = self.fc(feat)\n",
    "        return out\n",
    "\n",
    "# -------------------- 8. Main Pipeline --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 데이터 로드\n",
    "\n",
    "    df = pd.read_csv(PATH)\n",
    "    signal_cols = [c for c in df.columns if c != 'Label']\n",
    "    signals = df[signal_cols].values.astype(np.float32)\n",
    "    labels = df['Label'].values.astype(np.int64)\n",
    "\n",
    "    window_size, stride = 500, 250\n",
    "    segments, seg_labels, groups = [], [], []\n",
    "    for subj_id, (sig, lab) in enumerate(zip(signals, labels), start=1):\n",
    "        for start in range(0, len(sig)-window_size+1, stride):\n",
    "            segments.append(sig[start:start+window_size])\n",
    "            seg_labels.append(lab)\n",
    "            groups.append(subj_id)\n",
    "    segments = np.stack(segments)\n",
    "    seg_labels = np.array(seg_labels)\n",
    "    groups = np.array(groups)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # 1단계: Self-Supervised 학습\n",
    "    ssl_dataset = SSLDataset(segments, PPGAugmentation())\n",
    "    ssl_loader = DataLoader(ssl_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    model_ssl = DualEncoderSSL().to(device)\n",
    "    optimizer_ssl = torch.optim.Adam(model_ssl.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(1, 21):\n",
    "        model_ssl.train(); total_loss = 0\n",
    "        for v1, v2 in ssl_loader:\n",
    "            v1, v2 = v1.to(device), v2.to(device)\n",
    "            z1, z2 = model_ssl(v1, v2)\n",
    "            loss = nt_xent(z1, z2)\n",
    "            optimizer_ssl.zero_grad(); loss.backward(); optimizer_ssl.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"[SSL] Epoch {epoch:02d} — Loss: {total_loss/len(ssl_loader):.4f}\")\n",
    "\n",
    "    # 2단계: Fine-tuning\n",
    "    encoder = model_ssl.encoder\n",
    "    for p in encoder.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    unique_subjects = np.unique(groups)\n",
    "    np.random.shuffle(unique_subjects)\n",
    "    folds = np.array_split(unique_subjects, 10)\n",
    "\n",
    "    for fold_idx, test_subjects in enumerate(folds, start=1):\n",
    "        is_test = np.isin(groups, test_subjects)\n",
    "        X_tr, y_tr = segments[~is_test], seg_labels[~is_test]\n",
    "        X_te, y_te = segments[is_test], seg_labels[is_test]\n",
    "\n",
    "        train_loader = DataLoader(ClassifierDataset(X_tr, y_tr), batch_size=64, shuffle=True)\n",
    "        test_loader = DataLoader(ClassifierDataset(X_te, y_te), batch_size=64, shuffle=False)\n",
    "\n",
    "        model = FineTuneClassifier(encoder).to(device)\n",
    "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        for epoch in range(1, 11):\n",
    "            model.train()\n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(model(xb), yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        model.eval(); preds, truths = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in test_loader:\n",
    "                xb = xb.to(device)\n",
    "                preds.extend(model(xb).argmax(dim=1).cpu().numpy())\n",
    "                truths.extend(yb.numpy())\n",
    "\n",
    "        acc = accuracy_score(truths, preds)\n",
    "        print(f\"Fold {fold_idx} Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4ced9a-40f3-4f63-99f3-d3e1d676b5b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa16b429-b6a2-488d-8bb3-9713da8f0ac4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
